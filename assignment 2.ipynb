{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Erc2A41DMzku"
   },
   "source": [
    "# Введение в глубинное обучение, ФКН ВШЭ\n",
    "\n",
    "# Практическое задание 2. Рекуррентные Нейронные Сети\n",
    "\n",
    "## Общая информация\n",
    "Дата выдачи: 04.02.2021\n",
    "\n",
    "Мягкий дедлайн: 19.02.2021 00:59 MSK\n",
    "\n",
    "Жёсткий дедлайн: 22.02.2021 00:59 MSK\n",
    "\n",
    "## Оценивание и штрафы\n",
    "\n",
    "Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи). Максимально допустимая оценка за работу — 10 баллов.\n",
    "\n",
    "Сдавать задание после указанного срока сдачи нельзя. При выставлении неполного балла за задание в связи с наличием ошибок на усмотрение проверяющего предусмотрена возможность исправить работу на указанных в ответном письме условиях.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов. Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).\n",
    "\n",
    "\n",
    "## Формат сдачи\n",
    "Задания сдаются семинаристу на почту. Посылка должна содержать:\n",
    "* Ноутбук homework-practice-02-Username.ipynb\n",
    "\n",
    "Username — ваша фамилия на латинице"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqWw3aP7wgOL"
   },
   "source": [
    "## 0. Подготовка данных (0 баллов)\n",
    "\n",
    "Данные представляют собой корпус текстов с 4-мя категориями. Ваша задача - написать классификатор для этих данных, определяющий, к какой из категорий относится текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T16:58:04.931569Z",
     "start_time": "2021-03-01T16:58:04.202393Z"
    },
    "id": "o031RfNCWTvg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, RNN, LSTMCell, Activation, Dense, Dropout,Input, Embedding,SpatialDropout1D,Bidirectional,Conv1D, GlobalMaxPooling1D, BatchNormalization,Flatten\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import EarlyStopping\n",
    "#import torch\n",
    "#from torchtext import data\n",
    "#import torch.nn as nn\n",
    "%matplotlib inline\n",
    "\n",
    "df_train = pd.read_csv('./train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T16:58:07.216712Z",
     "start_time": "2021-03-01T16:58:06.639355Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "patterns = \"[A-Za-z0-9!#$%&'()*+,./:;<=>?@[\\]^_`{|}~—\\\"\\-]+\"\n",
    "stopwords_ru = stopwords.words(\"russian\")\n",
    "morph = MorphAnalyzer()\n",
    "def lemmatize(doc):\n",
    "    doc = re.sub(patterns, ' ', str(doc))\n",
    "    tokens = []\n",
    "    for token in doc.split():\n",
    "        if token and token not in stopwords_ru:\n",
    "            token = token.strip()\n",
    "            token = morph.normal_forms(token)[0]\n",
    "            \n",
    "            tokens.append(token)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T17:04:49.883655Z",
     "start_time": "2021-03-01T16:58:08.399836Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train['text'] = df_train['text'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T17:04:53.752605Z",
     "start_time": "2021-03-01T17:04:51.460981Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_train = df_train[df_train['text'] != None]\n",
    "list_none = []\n",
    "for i,row in df_train.iterrows():\n",
    "    if row['text'] == None:\n",
    "        list_none.append(i)\n",
    "list_none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T17:04:55.416559Z",
     "start_time": "2021-03-01T17:04:55.394226Z"
    }
   },
   "outputs": [],
   "source": [
    "#df_train.drop(df_train.index[list_none], inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bx2PZCCkQu91"
   },
   "source": [
    "## 1. Предобработка данных (2 балла)\n",
    "\n",
    "В этом задании вам предстоит провести предобработку данных. Баллы ставятся следующим образом:\n",
    "\n",
    "* Привести все тексты к одной длине, заменить слова/токены на числа, факторизовать целевую переменную и т.д. (1 балл)\n",
    "\n",
    "* Использовать токенизатор, который разбил бы все слова на токены (подробнее https://github.com/huggingface/tokenizers). (1 балл)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T17:04:57.467439Z",
     "start_time": "2021-03-01T17:04:57.410150Z"
    },
    "id": "ySV44oIeP_ri"
   },
   "outputs": [],
   "source": [
    "y = df_train['source']\n",
    "X = df_train['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T17:04:59.414074Z",
     "start_time": "2021-03-01T17:04:59.040903Z"
    },
    "id": "dEjU-VBIwDM6"
   },
   "outputs": [],
   "source": [
    "train_text,test_text,train_cat,test_cat = train_test_split(X,y,test_size=0.15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T17:05:03.970341Z",
     "start_time": "2021-03-01T17:05:01.252687Z"
    },
    "id": "jzYffM1aQYLB"
   },
   "outputs": [],
   "source": [
    "max_features = 5000\n",
    "max_len = 250\n",
    "tok = Tokenizer(num_words=max_features)\n",
    "tok.fit_on_texts(X)\n",
    "sequences = tok.texts_to_sequences(train_text)\n",
    "X_train = sequence.pad_sequences(sequences,maxlen=max_len,padding=\"post\")\n",
    "\n",
    "test_sequences = tok.texts_to_sequences(test_text)\n",
    "X_test = sequence.pad_sequences(test_sequences,maxlen=max_len,padding=\"post\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T17:05:05.228964Z",
     "start_time": "2021-03-01T17:05:05.201008Z"
    },
    "id": "xEVrsbpiwxPZ"
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_cat)\n",
    "y_train = encoder.transform(train_cat)\n",
    "y_test = encoder.transform(test_cat)\n",
    "y_train = to_categorical(y_train, 4)\n",
    "y_test = to_categorical(y_test, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-XmxjpiwgOV"
   },
   "source": [
    "## 2. LSTM-сеть (4 балла)\n",
    "\n",
    "В этом задании вам предстоит написать LSTM сеть __вручную__ (то есть без использования стандартных реализаций из keras / torch / tensorflow). Сама архитектура отлично расписана здесь: https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пытался реализовать несколько дней,облазил все статьи на towards, git и др.  но не нашел как это встроить в модели для торча )));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gjeg3rwEHCJp"
   },
   "source": [
    "## 3. Модель (2 балла)\n",
    "\n",
    "В этом задании вам предстоит объединить вашу сеть с несколькими другими слоями для создания итоговой модели классификатора (можно начать с самой базовой архитектуры, слой эмбеддингов - LSTM - выходной слой)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btd3Ewfu0R5L"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T17:05:07.406663Z",
     "start_time": "2021-03-01T17:05:06.116956Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MOrp4ywcRAjX",
    "outputId": "eaf0d979-9d98-41fe-aef3-908b509f8ab5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_20 (Embedding)     (None, 250, 256)          1280000   \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 82, 64)            114752    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_20 (Spatia (None, 82, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 26, 128)           57472     \n",
      "_________________________________________________________________\n",
      "bidirectional_20 (Bidirectio (None, 256)               263168    \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 4)                 1028      \n",
      "=================================================================\n",
      "Total params: 1,716,420\n",
      "Trainable params: 1,716,420\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features,256, input_length = X_train.shape[1]))\n",
    "model.add(Conv1D(64, 7, padding=\"valid\",activation=\"relu\",strides=3))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "\n",
    "#model.add(Conv1D(64, 7, padding=\"valid\",activation=\"relu\",strides=5))\n",
    "model.add(Conv1D(128, 7, padding=\"valid\",activation=\"relu\",strides=3))\n",
    "#model.add(Flatten())\n",
    "model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
    "#model.add(GlobalMaxPooling1D())\n",
    "#model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(4,activation='sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T17:08:54.679737Z",
     "start_time": "2021-03-01T17:05:08.997699Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L-8lnxIHR8Ie",
    "outputId": "8d537845-7e5f-42e9-cdae-60bd83bc5c3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "132/132 [==============================] - 91s 536ms/step - loss: 0.4261 - accuracy: 0.5497 - val_loss: 0.0675 - val_accuracy: 0.9663\n",
      "Epoch 2/3\n",
      "132/132 [==============================] - 67s 507ms/step - loss: 0.0619 - accuracy: 0.9689 - val_loss: 0.0581 - val_accuracy: 0.9697\n",
      "Epoch 3/3\n",
      "132/132 [==============================] - 67s 510ms/step - loss: 0.0520 - accuracy: 0.9723 - val_loss: 0.0580 - val_accuracy: 0.9717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x27053ab3160>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "model.fit(X_train, y_train, epochs=3 , verbose=1, batch_size = batch_size, validation_data=(X_test,y_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T17:09:00.118275Z",
     "start_time": "2021-03-01T17:08:56.306364Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 4s 76ms/step - loss: 0.0580 - accuracy: 0.9717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0579695850610733, 0.9716790318489075]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T21:13:17.633186Z",
     "start_time": "2021-02-27T21:08:40.467Z"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "predicts = model.predict(test['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24VNfcMWHaKJ"
   },
   "source": [
    "## 4. Обучение модели (4 балла)\n",
    "\n",
    "**Важно!** Public Leaderboard содержит только 30% тестовых данных, баллы за задание будут измеряться в соответствии со всеми тестовыми данными, так что после завершения соревнования Ваша позиция на leaderboard-е может поменяться.\n",
    "\n",
    "**Важно!** При использовании **СВОЕЙ** модели LSTM полученные баллы удваиваются.\n",
    "\n",
    "* val_accuracy > baseline: 1 балл\n",
    "\n",
    "* Ваша позиция на лидерборде между 25% (включительно) и 50% (невключительно) квантилями среди студентов, перебивших baseline: 2 балла\n",
    "\n",
    "* Ваша позиция на лидерборде между 50% и 75% квантилями среди студентов, перебивших baseline: 3 балла\n",
    "\n",
    "* Ваша позиция на лидерборде между 75% и 90% квантилями среди студентов, перебивших baseline: 4 балла\n",
    "\n",
    "* Ваша позиция на лидерборде больше 90% квантиля среди студентов, перебивших baseline: 5 баллов\n",
    "\n",
    "* Вы в топ-3 студентов на лидерборде: 10 баллов\n",
    "\n",
    "\n",
    "**Максимальный суммарный балл** - 15\n",
    "\n",
    "**Финальная оценка**: min(суммарный балл, 10), доп баллы можно использовать для увеличения оценок за самостоятельные работы (1 доп балл = +1 балл к самостоятельной)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "piece_of_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
